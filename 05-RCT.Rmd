# ¬øPor qu√© la asignaci√≥n aleatoria elimina el sesgo?

::: {.boxinfo}
**üéØ Objetivo del cap√≠tulo**

- ‚úîÔ∏è Entender qu√© se debe cumplir para que un experimento sea exitoso y reduzca el sesgo de selecci√≥n  
- ‚úîÔ∏è Conocer las diferentes formas de realizar asignaci√≥n aleatoria  
- ‚úîÔ∏è Traducir los resultados potenciales a una regresi√≥n lineal  

### üìö Lecturas{-}

- üìÑ **Paper Alert:** [When Should You Adjust Standard Errors for Clustering? (NBER)](https://www.nber.org/papers/w24003)
- üìÑ **Teor√≠a:** [Lectura 4. Cap√≠tulo 4 Bernal y Pe√±a (PDF)](https://www.dropbox.com/s/vxpgxt22pvphwx3/Capitulo%204%20Bernal%20y%20Pe%C3%B1a.pdf?dl=0)

:::

Tenemos el siguiente modelo de regresi√≥n lineal:

\[
Y = \alpha + \tau D + \varepsilon
\]

Donde:

- \( Y \) es el resultado (por ejemplo, salario),
- \( D \) es una variable binaria de tratamiento,
- \( \varepsilon \) incluye **motivaci√≥n**, que no observamos.


Supongamos que la verdadera relaci√≥n es:

\[
Y = \alpha + \tau D + \gamma M + u
\]

Donde:

- \( M \) es motivaci√≥n (no observable),
- \( u \) es un nuevo error sin correlaci√≥n con \( D \),
- Pero **no incluimos** \( M \) en la estimaci√≥n ‚Üí queda absorbido en \( \varepsilon = \gamma M + u \)

El modelo estimado es:

\[
Y = \alpha + \tau D + \varepsilon \quad \text{con} \quad \varepsilon = \gamma M + u
\]


Recordemos que el estimador de m√≠nimos cuadrados ordinarios es:

\[
\hat{\beta} = (X'X)^{-1} X'Y
\]

Con \( X = [\mathbf{1}, D] \), tenemos:

\[
\hat{\beta} = 
\begin{bmatrix}
\hat{\alpha} \\
\hat{\tau}
\end{bmatrix}
=
\left( \begin{bmatrix}
1 & D_1 \\
\vdots & \vdots \\
1 & D_n \\
\end{bmatrix}' 
\begin{bmatrix}
1 & D_1 \\
\vdots & \vdots \\
1 & D_n \\
\end{bmatrix} \right)^{-1}
\begin{bmatrix}
1 & D_1 \\
\vdots & \vdots \\
1 & D_n \\
\end{bmatrix}' Y
\]

Queremos analizar el **sesgo** en \(\hat{\tau}\). Sustituyendo \( Y = \alpha + \tau D + \gamma M + u \)

Entonces:

\[
\hat{\tau} = \tau + \gamma \cdot \frac{\text{Cov}(D, M)}{\text{Var}(D)}
\]

Interpretaci√≥n

- Si \( \text{Cov}(D, M) \neq 0 \), es decir, **si el tratamiento est√° correlacionado con la motivaci√≥n**, el estimador de \(\tau\) estar√° **sesgado**.
- El sesgo es proporcional a:
  - El efecto de la motivaci√≥n sobre \( Y \): \( \gamma \)
  - La correlaci√≥n entre \( D \) y \( M \): \( \text{Cov}(D, M) \)


**Resumen del sesgo**

::: {.table .table-bordered .table-striped}
| Correlaci√≥n entre \( D \) y \( M \) | Efecto de \( M \) sobre \( Y \) (\( \gamma \)) | ¬øHay sesgo en \( \hat{\tau} \)? | Direcci√≥n esperada del sesgo      |
|:----------------------------------:|:---------------------------------------------:|:-------------------------------:|:---------------------------------:|
| Cero                               | Cualquiera                                     | ‚ùå No                           | ‚Äì                                 |
| Positiva                           | Positiva                                       | ‚úÖ S√≠                          | \( \hat{\tau} > \tau \) (sesgo hacia arriba) |
| Positiva                           | Negativa                                       | ‚úÖ S√≠                          | \( \hat{\tau} < \tau \) (sesgo hacia abajo) |
| Negativa                           | Positiva                                       | ‚úÖ S√≠                          | \( \hat{\tau} < \tau \) (sesgo hacia abajo) |
| Negativa                           | Negativa                                       | ‚úÖ S√≠                          | \( \hat{\tau} > \tau \) (sesgo hacia arriba) |
:::

**Lectura de la tabla**:

- Si el tratamiento est√° **correlacionado positivamente** con la motivaci√≥n y la motivaci√≥n **aumenta** el resultado, el estimador de \( \tau \) estar√° **sesgado hacia arriba**.
- Si la motivaci√≥n est√° **omitida** y adem√°s est√° **correlacionada con el tratamiento**, siempre hay **sesgo**.
- Solo si la **motivaci√≥n no est√° correlacionada con el tratamiento**, aunque no la observemos, **no hay sesgo**.


## ¬øQu√© hace la aleatorizaci√≥n^?
üìå La **aleatorizaci√≥n** garantiza \( \text{Cov}(D, M) = 0 \), eliminando el sesgo de selecci√≥n sin necesidad de observar \( M \).

 *Visualizaci√≥n: motivaci√≥n, selecci√≥n y aleatorizaci√≥n*

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
library(ggplot2)
library(patchwork)

# Simular datos
N <- 1000
motivacion <- rnorm(N)
epsilon <- rnorm(N)
salario <- 10 + 3 * motivacion + epsilon

# Tratamiento con sesgo: m√°s motivados
tratamiento_sesgado <- ifelse(motivacion > 0, 1, 0)
# Tratamiento aleatorio
tratamiento_azar <- rbinom(N, 1, 0.5)

# Armar data frame
df <- data.frame(
  salario,
  motivacion,
  sesgado = factor(tratamiento_sesgado, labels = c("Control", "Tratado")),
  azar = factor(tratamiento_azar, labels = c("Control", "Tratado"))
)

# Gr√°fico 1: dispersi√≥n con auto-selecci√≥n
p1 <- ggplot(df, aes(x = motivacion, y = salario, color = sesgado)) +
  geom_point(alpha = 0.4) +
  labs(title = "Auto-selecci√≥n: motivaci√≥n no balanceada",
       color = "Grupo (sesgado)") +
  theme_minimal()

# Gr√°fico 2: dispersi√≥n con aleatorizaci√≥n
p2 <- ggplot(df, aes(x = motivacion, y = salario, color = azar)) +
  geom_point(alpha = 0.4) +
  labs(title = "Aleatorizaci√≥n: motivaci√≥n balanceada",
       color = "Grupo (azar)") +
  theme_minimal()

p1 + p2
```

---

## 4.2 Densidad de la motivaci√≥n por grupo

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Gr√°fico 3: densidad de motivaci√≥n seg√∫n tratamiento sesgado
dens1 <- ggplot(df, aes(x = motivacion, fill = sesgado)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribuci√≥n de motivaci√≥n (auto-selecci√≥n)",
       fill = "Grupo (sesgado)") +
  theme_minimal()

# Gr√°fico 4: densidad de motivaci√≥n seg√∫n tratamiento aleatorio
dens2 <- ggplot(df, aes(x = motivacion, fill = azar)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribuci√≥n de motivaci√≥n (aleatorizaci√≥n)",
       fill = "Grupo (azar)") +
  theme_minimal()

dens1 + dens2
```


Por lo tanto ya no es necesario observar la motivaci√≥n, ya que la aleatorizaci√≥n garantiza que los grupos sean comparables. Y la estimaci√≥n de  \(\tau\) ser√° insesgada y consistente en la siguiente regresi√≥n: 

\[
Y = \alpha + \tau D + u
\]

- Cual es el valor de \(\alpha\) y \(\tau\) en este caso?
- ¬øPor qu√© se incluyen controles en el an√°lisis de RCT?
- ¬øQu√© pasa si no se incluyen?
- ¬øCalcule los errores est√°ndar? 



::: {.boxvideo .green title="üé• Videos recomendados:"}


- [Video 1](https://www.youtube.com/embed/eGRd8jBdNYg)  
- [Video 2](https://www.youtube.com/embed/crpuBZv6XtA)  
- [Video 3](https://www.youtube.com/embed/xlX3VtuIfQ0)

Y todos los que encuentres en Google buscando: **RCT Esther Duflo**
:::


::: {.boxnote }

üõ†Ô∏è üí¨ **PROMPT DE CHATGPT PARA REFLEXI√ìN PROFUNDA**

**Instrucciones**: Copia este mensaje en ChatGPT o la IA de tu preferencia. Tu objetivo no es obtener respuestas, sino **reflexionar guiado por preguntas**.

---

Hola. Act√∫a como mi tutor metodol√≥gico. No quiero que me des respuestas. Quiero que me ayudes a pensar como si estuvi√©ramos en una tutor√≠a.

Estoy estudiando dise√±os experimentales. Entiendo que si el tratamiento se asigna aleatoriamente, entonces \( \text{Cov}(D, X) = 0 \), incluso para variables no observables.

Pero sigo viendo que en muchos papers experimentales incluyen **controles en la regresi√≥n**. Ay√∫dame a pensar **paso a paso** si eso es necesario o no.

Por favor, hazme preguntas como:

- ¬øQu√© gana o pierde la estimaci√≥n si incluyo controles?
- ¬øQu√© pasa si hay desequilibrios por azar?
- ¬øQu√© efecto tiene sobre la precisi√≥n del estimador?
- ¬øLos controles ayudan a mejorar algo aunque \( \hat{\tau} \) ya sea insesgado?
- ¬øHay casos en que incluir controles puede ser problem√°tico?

‚ö†Ô∏è No me des respuestas. Solo nuevas preguntas que me ayuden a entender mejor este punto.

:::


---
